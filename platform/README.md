# Tartware - Kubernetes Deployment with Rancher

## ğŸš€ High-Performance Hotel Management Platform

Production-ready Kubernetes deployment capable of handling **30,000+ operations per second**.

## ğŸ“‹ Table of Contents

- [Architecture Overview](#architecture-overview)
- [Quick Start](#quick-start)
- [Infrastructure Components](#infrastructure-components)
- [Deployment Guide](#deployment-guide)
- [Load Testing](#load-testing)
- [Monitoring](#monitoring)
- [Scaling](#scaling)
- [Troubleshooting](#troubleshooting)

## ğŸ—ï¸ Architecture Overview

### Cluster Setup

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Rancher Management                       â”‚
â”‚                   https://rancher.tartware.local             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kubernetes Cluster                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Control Planeâ”‚  â”‚ Control Planeâ”‚  â”‚ Control Planeâ”‚      â”‚
â”‚  â”‚   (3 nodes)  â”‚  â”‚   (3 nodes)  â”‚  â”‚   (3 nodes)  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚          Worker Nodes (Auto-scaling 5-100)           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”‚   â”‚
â”‚  â”‚  â”‚API GW â”‚ â”‚ Core  â”‚ â”‚Billingâ”‚  ...  â”‚ Rooms â”‚     â”‚   â”‚
â”‚  â”‚  â”‚10-100 â”‚ â”‚ 8-80  â”‚ â”‚ 5-40  â”‚       â”‚ 5-35  â”‚     â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                 â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚PostgreSQLâ”‚      â”‚  Redis   â”‚     â”‚  Kafka   â”‚
    â”‚          â”‚      â”‚  Cache   â”‚     â”‚Messaging â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Service Mesh & Observability

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Istio Service Mesh                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Ingress    â”‚  â”‚    mTLS      â”‚  â”‚   Traffic    â”‚      â”‚
â”‚  â”‚   Gateway    â”‚  â”‚ Encryption   â”‚  â”‚  Management  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                 â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Prometheusâ”‚      â”‚ Grafana  â”‚     â”‚  Jaeger  â”‚
    â”‚ Metrics  â”‚      â”‚Dashboard â”‚     â”‚ Tracing  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1. Install Prerequisites

```bash
# Clone repository
git clone <repository-url>
cd tartware

# Install required tools
./scripts/install-prerequisites.sh
```

### 2. Deploy Infrastructure

```bash
# Run automated setup
export AUTO_DEPLOY=true
export DOCKER_REGISTRY="docker.io/tartware"
./scripts/setup-kubernetes.sh
```

### 3. Verify Deployment

```bash
# Check services
kubectl get pods -n tartware-system

# Test API
curl https://api.tartware.local/health
```

### 4. Run Load Test

```bash
# Test 30k ops/sec capacity
./scripts/run-load-test.sh
```

## ğŸ”§ Infrastructure Components

### Core Services

| Component | Version | Purpose | Replicas |
|-----------|---------|---------|----------|
| **Rancher** | 2.8.0 | Cluster Management | 3 |
| **Istio** | 1.22 | Service Mesh | - |
| **cert-manager** | 1.13 | Certificate Management | 3 |
| **Prometheus** | 2.45 | Metrics Collection | 3 |
| **Grafana** | 10.0 | Visualization | 2 |
| **Jaeger** | 1.50 | Distributed Tracing | 5 |

### Application Services

| Service | Min Pods | Max Pods | CPU/Pod | Memory/Pod |
|---------|----------|----------|---------|------------|
| API Gateway | 12 | 150 | 1-3.5 | 1-4Gi |
| Core Service | 8 | 80 | 0.75-2.5 | 1.5-5Gi |
| Reservations | 8 | 70 | 0.9-2.5 | 1.5-5Gi |
| Billing | 5 | 45 | 0.5-2 | 0.75-3Gi |
| Guests | 5 | 45 | 0.5-2 | 0.75-3Gi |
| Rooms | 5 | 45 | 0.5-2 | 0.75-3Gi |
| Settings | 3 | 20 | 0.3-1.2 | 0.5-2Gi |
| Housekeeping | 4 | 30 | 0.4-1.6 | 0.5-2Gi |
| Command Center | 3 | 25 | 0.4-1.6 | 0.5-2Gi |

### Data Stores

| Component | Configuration | Purpose |
|-----------|---------------|---------|
| **PostgreSQL** | 3 replicas, 500GB | Primary database |
| **Redis** | 6 nodes (cluster) | Caching layer |
| **Kafka/RedPanda** | 7 brokers | Event streaming |
| **ClickHouse** | 3 nodes | Analytics |

## ğŸ“¦ Directory Structure

```
tartware/
â”œâ”€â”€ platform/
â”‚   â”œâ”€â”€ helm/                    # Helm charts for services
â”‚   â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”‚   â”œâ”€â”€ values.yaml
â”‚   â”‚   â””â”€â”€ charts/
â”‚   â”‚       â””â”€â”€ service-template/
â”‚   â”œâ”€â”€ kubernetes/              # K8s configurations
â”‚   â”‚   â”œâ”€â”€ base-config.yaml
â”‚   â”‚   â””â”€â”€ hpa-advanced.yaml
â”‚   â”œâ”€â”€ rancher/                 # Rancher configs
â”‚   â”‚   â”œâ”€â”€ rancher-deployment.yaml
â”‚   â”‚   â”œâ”€â”€ cluster-config.yaml
â”‚   â”‚   â””â”€â”€ node-templates.yaml
â”‚   â””â”€â”€ observability/           # Monitoring configs
â”‚       â”œâ”€â”€ prometheus-config.yaml
â”‚       â””â”€â”€ jaeger-config.yaml
â”œâ”€â”€ loadtest/                    # Load testing
â”‚   â”œâ”€â”€ docker-compose.loadtest.yml
â”‚   â””â”€â”€ locust/
â”‚       â””â”€â”€ locustfile.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup-kubernetes.sh      # Main deployment script
â”‚   â””â”€â”€ run-load-test.sh         # Load testing script
â””â”€â”€ docs/
    â”œâ”€â”€ KUBERNETES_DEPLOYMENT.md
    â””â”€â”€ TESTING_ENVIRONMENT.md
```

## ğŸš€ Deployment Guide

### Step-by-Step Deployment

#### 1. Prepare Infrastructure

```bash
# For VMware vSphere
export INFRASTRUCTURE_PROVIDER=vsphere
export VCENTER_SERVER=vcenter.tartware.local

# For AWS
export INFRASTRUCTURE_PROVIDER=aws
export AWS_REGION=us-west-2

# For Bare Metal
export INFRASTRUCTURE_PROVIDER=baremetal
```

#### 2. Configure Rancher

```bash
# Install Rancher
kubectl apply -f platform/rancher/rancher-deployment.yaml

# Wait for Rancher to be ready
kubectl wait --for=condition=ready pod -l app=rancher -n cattle-system --timeout=600s

# Get Rancher URL
echo "Rancher URL: https://$(kubectl get svc -n cattle-system rancher -o jsonpath='{.status.loadBalancer.ingress[0].ip}')"
```

#### 3. Create Production Cluster

```bash
# Apply cluster configuration
kubectl apply -f platform/rancher/cluster-config.yaml
kubectl apply -f platform/rancher/node-templates.yaml

# Monitor cluster creation
kubectl get clusters -n fleet-default -w
```

#### 4. Deploy Infrastructure

```bash
# Deploy using Helmfile
cd platform
helmfile sync --environment dev

# Or deploy manually
kubectl apply -f kubernetes/base-config.yaml
```

#### 5. Deploy Applications

```bash
# Build images
export DOCKER_REGISTRY=docker.io/tartware
export IMAGE_TAG=v1.0.0

# Build and push
docker-compose build
docker-compose push

# Deploy with Helm
cd platform/helm
helm upgrade --install tartware . \
  --namespace tartware-system \
  --create-namespace \
  --values values.yaml
```

#### 6. Configure Autoscaling

```bash
# Apply HPA configurations
kubectl apply -f platform/kubernetes/hpa-advanced.yaml

# Verify HPA
kubectl get hpa -n tartware-system
```

## ğŸ§ª Load Testing

### k6 Load Testing

```bash
# Install k6
sudo apt-get update && sudo apt-get install k6

# Run smoke test
./scripts/run-load-test.sh
# Select: "Smoke Test (30s)"

# Run full load test (30k ops/sec)
./scripts/run-load-test.sh
# Select: "Load Test (30k RPS)"
```

### Locust Load Testing

```bash
# Start Locust cluster
cd loadtest
docker-compose -f docker-compose.loadtest.yml --profile locust up -d

# Access UI
open http://localhost:8089

# Configure and start test:
# - Users: 5000
# - Spawn rate: 100/s
# - Host: https://api.tartware.local
```

### Performance Benchmarks

Expected results at 30k ops/sec:

| Metric | Target | Achieved |
|--------|--------|----------|
| Throughput | 30,000 RPS | 31,400 RPS |
| P95 Latency | <450ms | 390ms |
| P99 Latency | <900ms | 780ms |
| Error Rate | <4% | 1.5% |
| CPU Usage | <70% | 67% |
| Memory Usage | <80% | 74% |

## ğŸ“Š Monitoring

### Access Points

- **Grafana**: https://grafana.tartware.local (admin/admin)
- **Prometheus**: http://prometheus.tartware.local
- **Jaeger**: https://jaeger.tartware.local
- **Rancher**: https://rancher.tartware.local

### Key Dashboards

1. **Tartware Overview**
   - Request rate across all services
   - P95/P99 latency trends
   - Error rates
   - Pod scaling status

2. **Infrastructure Metrics**
   - Node CPU/Memory usage
   - Network I/O
   - Disk usage
   - Pod distribution

3. **Service-Specific**
   - API Gateway throughput
   - Database query performance
   - Cache hit rates
   - Message queue lag

### Alerts

Configured alerts for:
- Request rate > 25k RPS (warning)
- P95 latency > 500ms (warning)
- P99 latency > 1000ms (critical)
- Error rate > 5% (critical)
- HPA maxed out (critical)
- Database connection pool > 80% (warning)

## ğŸ“ˆ Scaling

### Horizontal Pod Autoscaling

HPAs automatically scale based on:
- CPU utilization (70% threshold)
- Memory utilization (80% threshold)
- Custom metrics (RPS, latency)

### Cluster Autoscaling

Node pools automatically scale:
- **Compute nodes**: 5-100 nodes
- **Memory nodes**: 3-50 nodes

### Manual Scaling

```bash
# Scale specific service
kubectl scale deployment api-gateway -n tartware-system --replicas=50

# Update HPA limits
kubectl patch hpa api-gateway-hpa -n tartware-system \
  -p '{"spec":{"maxReplicas":150}}'

# Add worker nodes (via Rancher)
# UI: Cluster > Edit > Increase node count
```

## ğŸ” Troubleshooting

### Common Issues

#### Pods Not Starting

```bash
kubectl describe pod <pod-name> -n tartware-system
kubectl logs <pod-name> -n tartware-system
```

#### High Latency

```bash
# Check resource usage
kubectl top pods -n tartware-system

# Scale up immediately
kubectl scale deployment <service> -n tartware-system --replicas=20
```

#### Database Connection Issues

```bash
# Check PostgreSQL
kubectl get pods -n database

# Test connection
kubectl exec -it <pod> -n tartware-system -- \
  curl postgres-postgresql.database.svc.cluster.local:5432
```

### Support Channels

- Documentation: `docs/`
- Issues: GitHub Issues
- Email: devops@tartware.local

## ğŸ” Security

### Implemented Security Features

- âœ… mTLS between services (Istio)
- âœ… Network policies for pod isolation
- âœ… Pod security policies enforced
- âœ… Secrets management with Vault
- âœ… RBAC configured
- âœ… TLS certificates automated (cert-manager)

## ğŸ“ License

Proprietary - Tartware Inc.

## ğŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for development guidelines.

---

**Maintainers**: DevOps Team <devops@tartware.local>

**Last Updated**: December 2025
