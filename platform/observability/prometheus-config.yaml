# Prometheus Custom Metrics and Alerting Rules for 30k ops/sec monitoring

apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-custom-rules
  namespace: observability
data:
  tartware-alerts.yaml: |
    groups:
      - name: tartware_performance_alerts
        interval: 15s
        rules:
          # High request rate alert
          - alert: HighRequestRate
            expr: sum(rate(http_requests_total[1m])) > 25000
            for: 1m
            labels:
              severity: warning
              component: api-gateway
            annotations:
              summary: "Request rate exceeding capacity"
              description: "Current request rate: {{ $value }} req/s"

          # API Gateway response time
          - alert: HighAPILatency
            expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{service="api-gateway"}[5m])) > 0.5
            for: 2m
            labels:
              severity: warning
              component: api-gateway
            annotations:
              summary: "API Gateway P95 latency is high"
              description: "P95 latency is {{ $value }}s"

          # Database connection pool saturation
          - alert: DatabasePoolSaturation
            expr: (database_connections_active / database_connections_max) > 0.8
            for: 5m
            labels:
              severity: critical
              component: database
            annotations:
              summary: "Database connection pool near capacity"
              description: "Pool usage: {{ $value }}%"

          # Memory pressure
          - alert: HighMemoryUsage
            expr: (container_memory_working_set_bytes / container_spec_memory_limit_bytes) > 0.85
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Container memory usage high"
              description: "Memory usage: {{ $value }}%"

          # Pod autoscaling lag
          - alert: HPAMaxedOut
            expr: kube_horizontalpodautoscaler_status_current_replicas == kube_horizontalpodautoscaler_spec_max_replicas
            for: 10m
            labels:
              severity: critical
            annotations:
              summary: "HPA has reached maximum replicas"
              description: "Service {{ $labels.horizontalpodautoscaler }} maxed out"

          # Error rate
          - alert: HighErrorRate
            expr: (sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))) > 0.05
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "High error rate detected"
              description: "Error rate: {{ $value }}%"

          # Queue depth (Kafka lag)
          - alert: KafkaConsumerLag
            expr: kafka_consumer_lag > 10000
            for: 5m
            labels:
              severity: warning
              component: messaging
            annotations:
              summary: "Kafka consumer lag is high"
              description: "Lag: {{ $value }} messages"

          # Reservations dead-letter queue warning threshold
          - alert: ReservationCommandDlqWarning
            expr: reservation_event_dlq_depth
              > max without (level) (reservation_event_dlq_threshold{level="warn"})
            for: 2m
            labels:
              severity: warning
              component: reservations-command-service
            annotations:
              summary: "Reservation dead-letter queue backlog is above the warning threshold"
              description: "Dead-letter queue depth {{ $value }} exceeds the warning threshold; inspect /health/reliability and Kafka consumers."

          # Reservations dead-letter queue critical threshold
          - alert: ReservationCommandDlqCritical
            expr: reservation_event_dlq_depth
              > max without (level) (reservation_event_dlq_threshold{level="critical"})
            for: 2m
            labels:
              severity: critical
              component: reservations-command-service
            annotations:
              summary: "Reservation dead-letter queue backlog exceeded the critical threshold"
              description: "Dead-letter queue depth {{ $value }} is above the critical limit; trigger remediation per runbook."

          # Roll service streaming lag > 60s
          - alert: RollServiceProcessingLag
            expr: max(roll_service_processing_lag_seconds) > 60
            for: 2m
            labels:
              severity: warning
              component: roll-service
            annotations:
              summary: "Roll service Kafka consumer is {{ $value }}s behind"
              description: "Shadow ledger ingestion is lagging reservations.events by more than 60 seconds."

          - alert: RollServiceEventTimestampDrift
            expr: max(roll_service_consumer_timestamp_drift_seconds{topic="reservations.events"}) > 300
            for: 5m
            labels:
              severity: critical
              component: roll-service
            annotations:
              summary: "Roll service has not processed fresh lifecycle events in >5 minutes"
              description: "Last processed reservations.events timestamp is {{ $value }} seconds old; investigate consumer health or Kafka connectivity."

          # Roll service consumer timestamp drift (no new events processed)
          - alert: RollServiceConsumerStale
            expr: (time() - max(roll_service_consumer_event_timestamp{topic="reservations.events"})) > 300
            for: 5m
            labels:
              severity: critical
              component: roll-service
            annotations:
              summary: "Roll service has not processed reservations.events for >5 minutes"
              description: "Last consumer event timestamp is {{ $value }} seconds old; investigate shadow pods or Kafka connectivity."

          # Backfill checkpoint not advancing (overnight job stuck)
          - alert: RollServiceBackfillStale
            expr: (time() - roll_service_backfill_checkpoint_timestamp) > 900
            for: 5m
            labels:
              severity: warning
              component: roll-service
            annotations:
              summary: "Roll service backfill checkpoint older than 15 minutes"
              description: "Shadow ledger backfill has not advanced; check batch job pods and Postgres."

          # Availability guard notification lag
          - alert: AvailabilityGuardNotificationLag
            expr: histogram_quantile(0.95, sum(rate(availability_guard_notification_delivery_lag_seconds_bucket[5m])) by (le)) > 60
            for: 2m
            labels:
              severity: warning
              component: availability-guard
            annotations:
              summary: "Manual release notifications are more than a minute behind"
              description: "P95 notification delivery lag is {{ $value }} seconds; inspect webhook endpoints or Kafka consumer health."

          # Availability guard per-channel failure spikes
          - alert: AvailabilityGuardNotificationFailures
            expr: (sum(increase(availability_guard_notification_channel_deliveries_total{status="failed"}[5m])) by (channel)) / clamp_min(sum(increase(availability_guard_notification_channel_deliveries_total[5m])) by (channel), 1) > 0.2
            for: 2m
            labels:
              severity: critical
              component: availability-guard
            annotations:
              summary: "Availability Guard {{ $labels.channel }} notifications are failing"
              description: "More than 20% of {{ $labels.channel }} deliveries failed in the last 5 minutes; check webhook credentials and downstream providers."

          # Service unavailability
          - alert: ServiceDown
            expr: up{job=~"tartware-.*"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Service is down"
              description: "{{ $labels.job }} is unreachable"

      - name: tartware_capacity_planning
        interval: 30s
        rules:
          # Capacity metrics
          - record: tartware:requests_per_second:sum
            expr: sum(rate(http_requests_total[1m]))

          - record: tartware:requests_per_second_by_service
            expr: sum(rate(http_requests_total[1m])) by (service)

          - record: tartware:api_latency_p95
            expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service))

          - record: tartware:api_latency_p99
            expr: histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service))

          - record: tartware:error_rate
            expr: sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))

          - record: tartware:pod_count_by_service
            expr: count(kube_pod_info{namespace="tartware-system"}) by (pod)

---
# ServiceMonitor for Prometheus to scrape services
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: tartware-services
  namespace: observability
  labels:
    release: kube-prometheus-stack
spec:
  selector:
    matchLabels:
      app.kubernetes.io/part-of: tartware
  namespaceSelector:
    matchNames:
      - tartware-system
  endpoints:
    - port: http
      path: /metrics
      interval: 15s
      scrapeTimeout: 10s
      relabelings:
        - sourceLabels: [__meta_kubernetes_pod_name]
          targetLabel: pod
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          targetLabel: node
        - sourceLabels: [__meta_kubernetes_namespace]
          targetLabel: namespace

---
# Grafana Dashboard ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-tartware
  namespace: observability
  labels:
    grafana_dashboard: "1"
data:
  tartware-overview.json: |
    {
      "dashboard": {
        "title": "Tartware - High-Performance Overview",
        "tags": ["tartware", "performance"],
        "timezone": "browser",
        "panels": [
          {
            "title": "Requests Per Second",
            "targets": [{
              "expr": "sum(rate(http_requests_total{namespace='tartware-system'}[1m]))"
            }],
            "type": "graph"
          },
          {
            "title": "P95 Response Time",
            "targets": [{
              "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service))"
            }],
            "type": "graph"
          },
          {
            "title": "Error Rate",
            "targets": [{
              "expr": "(sum(rate(http_requests_total{status=~'5..'}[5m])) / sum(rate(http_requests_total[5m]))) * 100"
            }],
            "type": "graph"
          },
          {
            "title": "Active Pods by Service",
            "targets": [{
              "expr": "count(kube_pod_info{namespace='tartware-system'}) by (created_by_name)"
            }],
            "type": "graph"
          },
          {
            "title": "CPU Usage by Service",
            "targets": [{
              "expr": "sum(rate(container_cpu_usage_seconds_total{namespace='tartware-system'}[5m])) by (pod)"
            }],
            "type": "graph"
          },
          {
            "title": "Memory Usage by Service",
            "targets": [{
              "expr": "sum(container_memory_working_set_bytes{namespace='tartware-system'}) by (pod)"
            }],
            "type": "graph"
          }
        ]
      }
    }
